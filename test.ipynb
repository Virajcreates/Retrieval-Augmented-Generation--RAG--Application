{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00444088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\joane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\joane\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "‚úÖ All libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from docx import Document as DocxDocument\n",
    "\n",
    "\n",
    "# üìÑ PDF/Text Parsing & Web Scraping\n",
    "import fitz  \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# üî¢ Vector Store\n",
    "import faiss\n",
    "\n",
    "# üß† LLMs\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# üß± RAG Chains (optional, but powerful)\n",
    "from langchain_community.vectorstores import FAISS as LC_FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa29f3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_pdf(file_path):\n",
    "    \"\"\"Extract text and metadata from PDF using PyMuPDF.\"\"\"\n",
    "    doc = fitz.open(file_path)\n",
    "    texts = []\n",
    "    for i, page in enumerate(doc):\n",
    "        page_text = page.get_text()\n",
    "        if page_text.strip():\n",
    "            texts.append({\n",
    "                \"text\": clean_text(page_text),\n",
    "                \"metadata\": {\n",
    "                    \"source\": os.path.basename(file_path),\n",
    "                    \"type\": \"pdf\",\n",
    "                    \"page\": i + 1\n",
    "                }\n",
    "            })\n",
    "    doc.close()\n",
    "    return texts\n",
    "\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    \"\"\"Extract text and metadata from TXT.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_text = f.read()\n",
    "    return [{\n",
    "        \"text\": clean_text(raw_text),\n",
    "        \"metadata\": {\n",
    "            \"source\": os.path.basename(file_path),\n",
    "            \"type\": \"txt\"\n",
    "        }\n",
    "    }]\n",
    "\n",
    "\n",
    "def load_docx_file(file_path):\n",
    "    \"\"\"Extract text and metadata from a DOCX (Word) file.\"\"\"\n",
    "    doc = DocxDocument(file_path)\n",
    "    full_text = \"\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n",
    "    return [{\n",
    "        \"text\": clean_text(full_text),\n",
    "        \"metadata\": {\n",
    "            \"source\": os.path.basename(file_path),\n",
    "            \"type\": \"docx\"\n",
    "        }\n",
    "    }]\n",
    "\n",
    "\n",
    "def load_webpage(url):\n",
    "    \"\"\"Extract cleaned webpage text + source metadata.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    raw_text = soup.get_text(separator=\"\\n\")\n",
    "    return [{\n",
    "        \"text\": clean_text(raw_text),\n",
    "        \"metadata\": {\n",
    "            \"source\": url,\n",
    "            \"type\": \"web\"\n",
    "        }\n",
    "    }]\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Basic text cleaning (unchanged).\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b32e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Document 1 ‚Äî Source: Report on IEEE DBCE Student Branch Meet.docx\n",
      "Report on IEEE DBCE Student Branch Meet & Greet Date: 26th March 2025 Organized by: IEEE Student Branch, Don Bosco College of Engineering (IEEE DBCE SB) The IEEE DBCE Student Branch conducted a Meet & Greet session to introduce committee members, foster networking, and plan future activities. The ev\n",
      "\n",
      "üìÑ Document 2 ‚Äî Source: Resume.pdf\n",
      "Ayden Xavier Alvito Joanes +91 9923577502 | joanesayden@gmail.com |   Ayden Joanes |   Ayden Joanes | Bengaluru, India Machine Learning Intern USP: A self-taught AI enthusiast and active swing trader driven to integrate deep learning with finance. I offer hands-on ML experience, creative thinking, a\n",
      "\n",
      "üìÑ Document 3 ‚Äî Source: Resume.pdf\n",
      "Fake News Detection Using XLM-RoBERTa and Microsoft Autogen with Dynamic Weighted Agents Tools: Python, Transformers, HuggingFace, XLM-RoBERTa, Microsoft Autogen, Pandas, scikit-learn   Objective: Built a flexible fake news detection system using a fine-tuned XLM-RoBERTa model and dynamic agents pow\n"
     ]
    }
   ],
   "source": [
    "#Load Multiple Documents from a Folder (PDF, TXT, DOCX) + URLs\n",
    "\n",
    "docs = []\n",
    "\n",
    "# üîπ Load from files\n",
    "input_dir = r\"C:\\Users\\joane\\OneDrive\\Desktop\\Sem 7\\RAG\\input_docs\"  # ‚Üê Put all your files here\n",
    "for filename in os.listdir(input_dir):\n",
    "    filepath = os.path.join(input_dir, filename)\n",
    "    if filename.lower().endswith(\".pdf\"):\n",
    "        docs.extend(load_pdf(filepath))\n",
    "    elif filename.lower().endswith(\".txt\"):\n",
    "        docs.extend(load_text_file(filepath))\n",
    "    elif filename.lower().endswith(\".docx\"):\n",
    "        docs.extend(load_docx_file(filepath))\n",
    "\n",
    "# üîπ Load from web pages (optional)\n",
    "web_urls = [\n",
    "    # \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
    "    # \"https://www.example.com/article.html\"\n",
    "]\n",
    "for url in web_urls:\n",
    "    docs.extend(load_webpage(url))\n",
    "\n",
    "# ‚úÖ Preview result\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\nüìÑ Document {i+1} ‚Äî Source: {doc['metadata']['source']}\")\n",
    "    print(doc['text'][:300])  # Show first 300 chars only\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba10cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunked into 22 total pieces.\n",
      "üìÑ Sample chunk:\n",
      " Report on IEEE DBCE Student Branch Meet & Greet Date: 26th March 2025 Organized by: IEEE Student Branch, Don Bosco College of Engineering (IEEE DBCE SB) The IEEE DBCE Student Branch conducted a Meet & Greet session to introduce committee members, foster networking, and plan future activities. The event aimed to strengthen collaboration, encourage active participation, and align with IEEE s mission of advancing technology for the benefit of society\n",
      "üìé Metadata: {'source': 'Report on IEEE DBCE Student Branch Meet.docx', 'type': 'docx', 'chunk': 1}\n"
     ]
    }
   ],
   "source": [
    "#Smart Chunking with Metadata Preservation\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Final container for all processed chunks\n",
    "chunked_docs = []\n",
    "\n",
    "# Go through each doc from the loader\n",
    "for doc in docs:\n",
    "    splits = text_splitter.split_text(doc[\"text\"])\n",
    "    for i, chunk in enumerate(splits):\n",
    "        chunked_docs.append(Document(\n",
    "            page_content=chunk,\n",
    "            metadata={**doc[\"metadata\"], \"chunk\": i + 1}  # retain metadata + chunk number\n",
    "        ))\n",
    "\n",
    "print(f\"‚úÖ Chunked into {len(chunked_docs)} total pieces.\")\n",
    "print(\"üìÑ Sample chunk:\\n\", chunked_docs[0].page_content)\n",
    "print(\"üìé Metadata:\", chunked_docs[0].metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c0ce05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS index created and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings for the chunks and store in FAISS vector DB\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"  # lightweight, fast, and accurate\n",
    ")\n",
    "\n",
    "# Create a FAISS vector store from the documents\n",
    "vector_store = LC_FAISS.from_documents(chunked_docs, embedding_model)\n",
    "\n",
    "# Save the vector store locally\n",
    "vector_store.save_local(\"vector_store\")\n",
    "\n",
    "print(\"‚úÖ FAISS index created and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05270f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚ùì Question: What is Ayden's area of interest?\n",
      "üß† Answer: Deeply intrigued by large language models, autonomous multi-agent systems, and behavioral psychology in tech. Enthusiastic about equity markets, human-centered design, and exploring startup ecosystems\n",
      "üìé Sources: ['Report on IEEE DBCE Student Branch Meet.docx', 'Resume.pdf']\n",
      "============================================================\n",
      "üëã Exiting the Q&A session...\n"
     ]
    }
   ],
   "source": [
    "#Load the saved FAISS index + Ask questions using a generator model\n",
    "\n",
    "import torch\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "\n",
    "# (1) Prevent download issues\n",
    "requests.adapters.DEFAULT_RETRIES = 5\n",
    "\n",
    "# (2) Load vector store\n",
    "retriever = LC_FAISS.load_local(\n",
    "    \"vector_store\",\n",
    "    embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ").as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# (3) Load the FLAN-T5 model\n",
    "generator = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-base\",\n",
    "    tokenizer=\"google/flan-t5-base\",\n",
    "    max_length=512,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    revision=\"main\"\n",
    ")\n",
    "\n",
    "# Ask a question (with optional source metadata)\n",
    "def ask_question(query, return_sources=False):\n",
    "    # 1. Retrieve relevant chunks\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    # 2. Construct the prompt\n",
    "    prompt = (\n",
    "        f\"Answer the following question using only the context below:\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {query}\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    # 3. Generate answer\n",
    "    response = generator(prompt, do_sample=False)[0]['generated_text']\n",
    "    answer = response.strip()\n",
    "\n",
    "    # 4. Optionally return sources\n",
    "    if return_sources:\n",
    "        sources = list({doc.metadata.get('source', 'Unknown') for doc in retrieved_docs})\n",
    "        return answer, sources\n",
    "    else:\n",
    "        return answer\n",
    "\n",
    "import time\n",
    "\n",
    "# Ask interactively from user input\n",
    "while True:\n",
    "    user_query = input(\"\\nüîç Ask a question (or type 'exit' to quit): \")\n",
    "    \n",
    "    if user_query.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"üëã Exiting the Q&A session...\")\n",
    "        time.sleep(0.5)\n",
    "        break\n",
    "\n",
    "    answer, citations = ask_question(user_query, return_sources=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"‚ùì Question: {user_query}\")\n",
    "    print(f\"üß† Answer: {answer}\")\n",
    "    print(f\"üìé Sources: {citations}\")\n",
    "    print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
